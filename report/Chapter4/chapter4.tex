% chapter4.tex

\chapter{Experimental Results and Discussion}

All 13 models (7 ML + 6 DL) trained on identical preprocessed data (80K training, 20K validation samples).

\section{Machine Learning Results}

\begin{table}[h]
\centering
\caption{ML Models: Performance Metrics}
\label{tab:ml_results}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Train Acc} & \textbf{Val Acc} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\hline
\textbf{LightGBM} & \textbf{67.15\%} & \textbf{62.94\%} & \textbf{0.6299} & \textbf{0.6294} & \textbf{0.6286} \\
Random Forest & 92.54\% & 62.09\% & 0.6222 & 0.6209 & 0.6192 \\
AdaBoost & 61.11\% & 61.26\% & 0.6143 & 0.6126 & 0.6104 \\
Decision Tree & 63.52\% & 60.10\% & 0.6025 & 0.6010 & 0.5986 \\
Logistic Reg. & 59.72\% & 60.07\% & 0.6017 & 0.6007 & 0.5988 \\
Naive Bayes & 55.36\% & 55.06\% & 0.5792 & 0.5506 & 0.4996 \\
SGD & 49.50\% & 49.46\% & 0.4644 & 0.4946 & 0.3283 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../beamer/figures/model_comparison.png}
\caption{Comprehensive Model Comparison: LightGBM achieves highest validation accuracy (62.94\%), with all DL models clustering tightly between 61.45-61.79\%.}
\label{fig:model_comparison}
\end{figure}

\subsection{Key ML Findings}

\textbf{LightGBM (Best - 62.94\%)}: Gradient boosting with L1/L2 regularization (200 trees, lr=0.1, max\_depth=5). Minimal train-val gap (4.21\%) indicates good generalization. Balanced precision/recall. Sequential error correction, histogram-based splitting, leaf-wise growth captures interactions, handles mixed types natively.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../beamer/figures/confusion_matrix_lightgbm.png}
\caption{LightGBM Confusion Matrix: Balanced performance across both classes with 62.94\% overall accuracy.}
\label{fig:confusion_matrix}
\end{figure}

\textbf{Random Forest (62.09\%)}: Severe overfitting (92.54\% train vs. 62.09\% val = 30.45\% gap). Bootstrap sampling limits capacity vs. boosting.

\textbf{AdaBoost (61.26\%)}: Minimal overfitting (61.11\% train vs. 61.26\% val). Adaptive weighting.

\textbf{Linear Models (60\%)}: Decision Tree, Logistic Regression cluster ~60\%, representing ceiling for linear/simple models with weak correlations.

\textbf{Naive Bayes (55.06\%)}: Independence assumption violated.

\textbf{SGD (49.46\%)}: Catastrophic failure near random.

\section{Deep Learning Results}

Six PyTorch architectures with AdamW, early stopping (patience=15), Apple MPS GPU.

\begin{table}[h]
\centering
\caption{DL Models: Performance Metrics}
\label{tab:dl_results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Parameters} & \textbf{Val Acc} & \textbf{Val Loss} & \textbf{F1} \\
\hline
\textbf{Deep MLP} & \textbf{63,714} & \textbf{61.79\%} & \textbf{0.6537} & \textbf{0.6130} \\
Residual Net & 418,306 & 61.62\% & 0.6545 & 0.6102 \\
Simple MLP & 60,738 & 61.61\% & 0.6535 & 0.6109 \\
Wide \& Deep & 60,890 & 61.52\% & 0.6541 & 0.6126 \\
Attention Net & 1,599,490 & 61.45\% & 0.6551 & 0.6118 \\
FT-Transformer & 38,722 & 61.45\% & 0.6545 & 0.6133 \\
\hline
\end{tabular}
\end{table}

\subsection{Key DL Findings}

\textbf{Deep MLP (Best - 61.79\%, 63K params)}: 256→128→64→32 with batch norm, ReLU, dropout(0.3). BatchNorm stabilizes training.

\textbf{Residual Net (61.62\%, 418K)}: 6.6x more parameters → only 0.17\% worse. Dataset limitations, not model limitations.

\textbf{Simple MLP (61.61\%, 60K)}: Nearly identical to Deep MLP without batch norm. Confirms dataset ceiling.

\textbf{Attention Net (61.45\%, 1.6M)}: 25x parameters, 0.34\% worse. Attention for sequential data irrelevant for unordered tabular features.

\textbf{FT-Transformer (61.45\%, 38K)}: Most parameter-efficient. Per-feature tokenization.

\textbf{DL Clustering}: All 6 within 0.34\% (61.45-61.79\%)—architecture irrelevant.

\section{ML vs. DL Comparison}

\begin{table}[h]
\centering
\caption{Best ML vs. Best DL}
\label{tab:ml_vs_dl}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Metric} & \textbf{LightGBM} & \textbf{Deep MLP} & \textbf{Difference} \\
\hline
Val Accuracy & 62.94\% & 61.79\% & +1.15\% \\
F1-Score & 0.6286 & 0.6130 & +0.0156 \\
Training Time & ~2 min & ~8 min & 4x faster \\
Interpretability & High & Low & ML better \\
Deployment & Simple & Complex & ML easier \\
\hline
\end{tabular}
\end{table}

\subsection{Why ML Outperforms}

\begin{enumerate}
    \item \textbf{Tabular Nature}: Heterogeneous features, no spatial/temporal structure. Trees handle mixed types; NNs struggle.
    \item \textbf{Sample Size}: 80K moderate. Trees efficient; DL needs millions.
    \item \textbf{Interactions}: Tree splits capture interactions; gradient boosting corrects errors; NNs struggle with weak correlations (max 0.118).
    \item \textbf{Inductive Bias}: Trees have strong tabular bias; NNs weak bias, need more data.
    \item \textbf{Robustness}: Trees robust to outliers, no scaling needed, handle missing natively.
\end{enumerate}

\textbf{DL Would Excel If}: 1M+ samples, hierarchical structure, richer interactions, transfer learning, raw byte analysis. None apply here.

\section{Kaggle Comparison}

Our best: 62.94\%. Kaggle top: 69.6\%. Gap: 6.66\%. Top submissions: extensive feature engineering, ensembles, exhaustive tuning. The 69.6\% ceiling confirms ~30\% irreducible error from weak correlations.

\section{Summary}

\textbf{Key Results}:
\begin{itemize}
    \item Best Overall: LightGBM 62.94\%
    \item Best DL: Deep MLP 61.79\%
    \item ML>DL: +1.15\%
    \item All DL within 0.34\%—architecture matters little
    \item ML spread: 13.48\% (best to worst)
\end{itemize}

\textbf{Research Questions}:
\begin{enumerate}
    \item ML outperforms DL by 1.15\% for tabular cybersecurity data
    \item Simple preprocessing sufficient (mean/mode imputation, label encoding, scaling)
    \item DL provides no advantage (6 architectures, 38K-1.6M params, all ~61.5\%)
    \item 41x parameter increase → 0.34\% gain (sharply diminishing returns)
    \item Architecture choice matters little for this dataset
\end{enumerate}

\textbf{Implications}: Prefer tree ensembles for tabular cybersecurity. DL vision/NLP advantages don't transfer. Trees provide interpretability (feature importance, rules). ML 4x faster, simpler deployment. Data quality trumps model sophistication.
