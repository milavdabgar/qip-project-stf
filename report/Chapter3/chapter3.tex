% chapter3.tex

\chapter{Experimental Setup and Model Selection}

\section{Data Preprocessing}

\textbf{Imputation}: Mean for 47 numerical features, mode for 28 categorical features.

\textbf{Encoding}: LabelEncoder for all categorical features (avoids dimensionality explosion from one-hot with 1,000+ categories).

\textbf{Scaling}: StandardScaler for numerical features: $z = \frac{x - \mu}{\sigma}$. Required for neural networks and distance-based methods.

\textbf{Split}: Stratified 80/20 (80K train, 20K validation), seed=42, maintains 50.52\% class distribution.

\section{Machine Learning Models}

Seven algorithms representing diverse paradigms:

\textbf{Decision Tree}: Baseline for tree methods, interpretable, handles mixed types. \textit{Hyperparameters}: max\_depth=10, min\_samples\_split=5, min\_samples\_leaf=2.

\textbf{Random Forest}: Ensemble with bootstrap sampling, reduces overfitting. \textit{Hyperparameters}: n\_estimators=100, max\_depth=20.

\textbf{LightGBM}: State-of-the-art gradient boosting, sequential error correction, histogram-based splits, expected top performer. \textit{Hyperparameters}: n\_estimators=200, learning\_rate=0.1, max\_depth=5, subsample=0.6, reg\_alpha=1.0, reg\_lambda=0.75.

\textbf{Naive Bayes}: Probabilistic baseline testing independence assumption with weak correlations. \textit{Hyperparameters}: var\_smoothing=1e-9.

\textbf{Logistic Regression}: Linear model baseline. \textit{Hyperparameters}: C=1.0, solver='liblinear', max\_iter=1000.

\textbf{AdaBoost}: Alternative boosting with adaptive re-weighting. \textit{Hyperparameters}: n\_estimators=200, learning\_rate=1.0.

\textbf{SGD Classifier}: Stochastic gradient descent, stress test. \textit{Hyperparameters}: loss='log\_loss', penalty='elasticnet', alpha=0.001.

\section{Deep Learning Models}

Six PyTorch architectures (38K-1.6M parameters):

\textbf{Simple MLP} (60,738 params): 75→256→128→64→32→2, ReLU, Dropout(0.3). Baseline feedforward network.

\textbf{Deep MLP} (63,714 params): Same architecture + BatchNorm after each layer. Expected best DL model.

\textbf{Residual Net} (418,306 params): 3 residual blocks with skip connections (256-dim). Tests hierarchical learning.

\textbf{Attention Net} (1,599,490 params): 2 multi-head self-attention blocks (4 heads), LayerNorm. Tests feature interaction learning.

\textbf{Wide \& Deep} (60,890 params): Wide (75→2) + Deep (75→256→128→64→2) paths. Combines memorization and generalization.

\textbf{FT-Transformer} (38,722 params): Per-feature tokens (75×64-dim), 3 encoder blocks (8 heads). State-of-the-art tabular architecture.

\subsection{Training Configuration}

PyTorch 2.5.1 on Apple M4 with MPS GPU (3-5x speedup). AdamW optimizer (lr=0.001, weight\_decay=1e-5), CrossEntropyLoss, batch size=512 (156 train batches, 39 val batches per epoch).

\textbf{Training}: Max 100 epochs with early stopping (patience=15 on validation accuracy). ReduceLROnPlateau scheduler (factor=0.5, patience=5). Save best checkpoint, load for final evaluation.

\textbf{Regularization}: Dropout(0.3), BatchNorm, weight decay (1e-5), early stopping.

\textbf{Metrics}: Accuracy (primary), F1-score, precision, recall. Seeds fixed (42) for reproducibility.

All models trained on identical preprocessed data enabling fair ML vs. DL comparison.