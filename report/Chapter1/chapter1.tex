% chapter1.tex 

\chapter{Introduction}

\section{Background and Motivation}

Modern organizations face over 200,000 new malware samples daily. Traditional signature-based detection systems, which rely on pattern matching against known malware databases, have become ineffective against sophisticated threats. Zero-day attacks—exploiting previously unknown vulnerabilities—circumvent signature databases entirely, creating critical security gaps.

Machine learning and deep learning offer a paradigm shift from reactive to proactive defense by analyzing behavioral patterns rather than static signatures. This project investigates which approach—traditional machine learning or modern deep learning—provides superior performance for malware detection through comprehensive experimentation using the Kaggle Microsoft Malware Prediction dataset, comparing 7 ML algorithms against 6 DL architectures.

\subsection{The ML vs. DL Debate for Tabular Data}

While deep learning has revolutionized computer vision and NLP, its effectiveness on tabular cybersecurity data remains an open question. Recent research suggests tree-based ensemble methods often outperform neural networks on structured data lacking spatial or temporal patterns. This work addresses this gap by implementing state-of-the-art models from both paradigms under identical conditions: same dataset, preprocessing, validation strategy, and metrics.

\section{Problem Statement and Research Questions}

Developing accurate malware prediction systems faces multiple challenges:

\begin{enumerate}
    \item \textbf{High Dimensionality}: 75 heterogeneous features spanning hardware, OS, security software, and behavioral indicators requiring careful preprocessing.
    
    \item \textbf{Weak Correlations}: Maximum feature-target correlation is only 0.118, necessitating complex interaction modeling.
    
    \item \textbf{Missing Data}: 1-30\% missing values, with security software attributes particularly incomplete.
    
    \item \textbf{Mixed Data Types}: 47 numerical and 28 categorical variables with cardinalities ranging from 2 to 1,000+.
    
    \item \textbf{Performance Ceiling}: Top Kaggle leaderboard reaches only 69.6\%, indicating high irreducible error.
\end{enumerate}

\subsection{Research Questions}

\begin{enumerate}
    \item How do traditional ML algorithms compare against DL architectures for tabular malware prediction?
    
    \item Does increasing neural network complexity (60K to 1.6M parameters) provide meaningful performance improvements?
    
    \item What are the practical trade-offs in training time, computational resources, deployment complexity, and interpretability?
\end{enumerate}

\section{Objectives and Contributions}

\subsection{Primary Objectives}

\begin{enumerate}
    \item \textbf{Comprehensive Implementation}: Train 13 diverse models—7 ML algorithms (Decision Tree, Random Forest, LightGBM, Naive Bayes, Logistic Regression, AdaBoost, SGD) and 6 DL architectures (Simple MLP, Deep MLP, Residual Net, Attention Net, Wide \& Deep, FT-Transformer).
    
    \item \textbf{Systematic Evaluation}: Conduct controlled comparison using identical train-validation splits, preprocessing, and metrics (accuracy, F1-score, precision, recall).
    
    \item \textbf{Production Deployment}: Build full-stack web application at \url{https://stf.milav.in} with all 13 trained models, interactive comparison, and live predictions.
    
    \item \textbf{Reproducible Research}: Document complete methodology, hyperparameters, and training procedures.
\end{enumerate}

\subsection{Key Contributions}

\begin{enumerate}
    \item \textbf{Empirical Validation}: Provides rigorous evidence that gradient boosting (LightGBM: 62.94\%) outperforms deep learning (best: 61.79\%) by 1.15\% with 4x faster training.
    
    \item \textbf{Architecture Analysis}: Demonstrates that increasing DL complexity from 38K to 1.6M parameters yields only 0.34\% performance variation, confirming dataset quality—not model sophistication—determines performance ceiling.
    
    \item \textbf{Practical Guidelines}: For tabular cybersecurity data with moderate samples (100K), heterogeneous features, and weak correlations, prioritize tree-based ensembles for superior accuracy, faster training, and better interpretability.
    
    \item \textbf{Production System}: Delivers fully functional web application bridging academic research and practical deployment.
\end{enumerate}