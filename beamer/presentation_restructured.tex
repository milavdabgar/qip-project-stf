% System Threat Forecaster Presentation
\documentclass[aspectratio=169]{beamer}

% Theme
\usetheme{Madrid}
\usecolortheme{default}

% Packages
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}

% Title Information
\title{System Threat Forecaster}
\subtitle{AICTE QIP PG Certification Programme on\\``Deep Learning: Fundamentals and Applications''}
\author{Milav Jayeshkumar Dabgar}
\institute{%
    Government Polytechnic Palanpur\\
    Department of Electronics and Communication Engineering
}
\date{December 2025}

% Enable speaker notes on second screen (for presentation mode)
\setbeameroption{show notes on second screen=right}
% Alternative: Use \setbeameroption{show notes} to show notes below slides

\begin{document}

% Title Slide
\begin{frame}
\titlepage
\end{frame}

\note[itemize]{
\item \textbf{[30 sec]} Good morning/afternoon everyone. My name is Milav Dabgar from Government Polytechnic Palanpur.
\item Today I'll present my QIP project on System Threat Forecaster - a machine learning approach to malware detection.
\item This 15-minute presentation covers our complete journey from problem identification to production deployment.
\item I'm grateful to my advisors Dr. Sarvaiya, Dr. Upla, Dr. Captain, and Dr. Deb for their guidance.
}

% Outline
\begin{frame}{Outline}
\begin{columns}[t]
\column{0.5\textwidth}
\tableofcontents[sections={1-3}]

\column{0.5\textwidth}
\tableofcontents[sections={4-6}]
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[15 sec]} Quick overview of our agenda:
\item We'll start with the problem context, move through our methodology and experiments, and conclude with deployment and findings.
\item This structure reflects the complete ML lifecycle from problem to production.
}

% Section 1: Project Context
\section{Project Context \& Learning Objectives}

\subsection{Problem Statement}
\begin{frame}{Problem Statement: Objectives \& Challenges}
\begin{alertblock}{Goal}
Predict malware infections and compare ML vs DL performance on tabular data
\end{alertblock}

\vspace{0.3cm}

\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Key Objectives:}
\begin{enumerate}
    \item Kaggle System Threat Forecaster
    \item Implement 7 ML algorithms
    \item Build 6 DL architectures
    \item ML vs DL comparison
    \item Full-stack deployment
    \item Production web app
\end{enumerate}

\column{0.5\textwidth}
\textbf{Key Challenges:}
\begin{itemize}
    \item \textbf{Top leaderboard:} 69.6\%
    \item High dimensionality (75 features)
    \item Weak correlations (max 0.118)
    \item High irreducible error (30\%+)
    \item Missing values in critical features
    \item 100K samples, balanced classes
\end{itemize}
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[1 min 15 sec]} This project tackles the Kaggle System Threat Forecaster competition.
\item Our goal was ambitious: implement 7 ML algorithms, 6 DL architectures, and deploy a production web application.
\item Key challenge: The competition's top score is only 69.6\% - indicating fundamental data limitations.
\item Notice the weak correlations - maximum only 0.118. This is crucial context for understanding our results.
\item With 100K balanced samples and 75 features, this looks like a typical ML problem, but the weak signals make it extremely challenging.
\item High irreducible error of 30\%+ means perfect classification is impossible with current features.
}

% Section 2: Data & Methodology
\section{Data \& Methodology}

\subsection{Dataset Overview}
\begin{frame}{Dataset: Kaggle - System Threat Forecaster Competition}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Data Characteristics:}
\begin{itemize}
    \item \textbf{Size:} 100,000 samples
    \item \textbf{Features:} 75 total
    \begin{itemize}
        \item 47 numerical
        \item 28 categorical
    \end{itemize}
    \item \textbf{Target:} Binary (malware: yes/no)
    \item \textbf{Balance:} 50.52\% / 49.48\%
    \item \textbf{Split:} 80/20 train-validation (stratified)
\end{itemize}

\vspace{0.3cm}

\textbf{Critical Insight:}
\begin{alertblock}{Data Quality}
\textbf{Weak correlations} (max 0.118) + High noise = Performance ceiling ~63\%
\end{alertblock}

\column{0.5\textwidth}
\includegraphics[width=\textwidth,height=0.7\textheight,keepaspectratio]{figures/feature_correlation.png}
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[45 sec]} Let's look at our dataset characteristics.
\item 100,000 samples with 75 features - 47 numerical and 28 categorical.
\item Target is binary: malware present or not. Classes are nearly balanced at 50-50.
\item The correlation heatmap reveals the core challenge - maximum correlation is just 0.118.
\item This weak correlation explains why even top Kaggle performers can't exceed 70\% accuracy.
\item We used stratified 80-20 split to maintain class balance in validation.
}

\subsection{Methodology}
\begin{frame}{Experimental Methodology \& Pipeline}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Preprocessing Steps:}
\begin{enumerate}
    \item \textbf{Missing Values:}
    \begin{itemize}
        \item Mean imputation (numerical)
        \item Mode imputation (categorical)
    \end{itemize}
    \item \textbf{Encoding:} LabelEncoder for categorical
    \item \textbf{Scaling:} StandardScaler for numerical
    \item \textbf{Validation:} Stratified K-Fold
\end{enumerate}

\vspace{0.3cm}

\textbf{Evaluation Metrics:}
\begin{itemize}
    \item Accuracy
    \item F1 Score (primary)
    \item Precision \& Recall
    \item Confusion Matrix
\end{itemize}

\column{0.5\textwidth}
\textbf{Model Development:}
\begin{enumerate}
    \item \textbf{ML:} 7 algorithms (scikit-learn)
    \item \textbf{DL:} 6 architectures (PyTorch)
    \item \textbf{GPU:} Apple MPS optimization
    \item \textbf{Tuning:} Grid search + validation
    \item \textbf{Goal:} ML vs DL comparison
\end{enumerate}

\vspace{0.3cm}

\begin{block}{Reproducibility}
Random seed: 42 | Version control: Git | Config management
\end{block}
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[1 min]} Our methodology follows ML best practices.
\item Preprocessing: Mean imputation for numerical features, mode for categorical. LabelEncoder for categories, StandardScaler for numerical.
\item We evaluated models using accuracy, F1 score, precision, and recall. F1 was our primary metric due to balanced classes.
\item Implemented 7 ML algorithms from scikit-learn and 6 DL architectures in PyTorch from scratch.
\item Used Apple M4's MPS acceleration for GPU training.
\item All experiments reproducible with seed 42 and version control.
\item Our goal: rigorous ML vs DL comparison on tabular data.
}

% Section 3: Model Experimentation
\section{Model Experimentation \& Learning}

\subsection{Machine Learning}
\begin{frame}{Machine Learning: 7 Algorithms Explored}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Algorithms Implemented:}
\begin{enumerate}
    \item \textbf{LightGBM} - 62.94\% (Winner!)
    \item Random Forest - 62.09\%
    \item AdaBoost - 61.26\%
    \item Decision Tree - 60.10\%
    \item Logistic Regression - 60.07\%
    \item Naive Bayes - 55.06\%
    \item SGD Classifier - 49.46\%
\end{enumerate}

\column{0.5\textwidth}
\textbf{Key Insights:}
\begin{itemize}
    \item \textbf{Gradient boosting} best for tabular data
    \item \textbf{Hyperparameter impact:}
    \begin{itemize}
        \item Learning rate: 0.1 optimal
        \item Max depth: 5 prevents overfitting
        \item Regularization crucial
    \end{itemize}
    \item \textbf{Ensemble} methods superior
    \item \textbf{F1 score} more informative than accuracy
\end{itemize}

\begin{block}{Performance Context}
62.94\% vs Kaggle top 69.6\% = 6.7\% gap indicates high dataset noise
\end{block}
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[1 min 15 sec]} Now the results - this is where theory met reality.
\item LightGBM emerged as clear winner at 62.94\% accuracy with F1 of 0.6286.
\item Notice the pattern: gradient boosting methods dominate. LightGBM, Random Forest, and AdaBoost are top 3.
\item Decision trees, logistic regression around 60\% - respectable but not exceptional.
\item SGD classifier at 49\% reminds us that not all algorithms suit all problems.
\item Key insight: 62.94\% vs Kaggle's top 69.6\% - only 6.7\% gap despite our simpler approach.
\item This validates that the dataset quality, not model complexity, is the limiting factor.
\item Hyperparameters matter: learning rate 0.1, max depth 5, and regularization were crucial for preventing overfitting.
}

\begin{frame}{ML Model Performance Comparison}
\begin{center}
\includegraphics[width=0.85\textwidth]{figures/model_comparison.png}
\end{center}

\vspace{0.2cm}
\begin{block}{Key Results}
\textbf{Best ML:} LightGBM 62.94\% (F1: 0.6286) | \textbf{Best DL:} Deep MLP 61.79\% (F1: 0.6130) | \textbf{Kaggle Top:} 69.6\%
\end{block}
\end{frame}

\note[itemize]{
\item \textbf{[30 sec]} This chart visualizes the performance hierarchy clearly.
\item LightGBM and Deep MLP lead the pack. Notice how close DL gets to ML - just 1.15\% difference.
\item All models cluster between 50-63\%, with outliers like SGD struggling.
\item The performance ceiling around 63\% appears consistent across approaches - strong evidence of dataset limitations.
}

\begin{frame}{Best ML Model: LightGBM Performance}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Confusion Matrix:}
\includegraphics[width=\textwidth]{figures/confusion_matrix_lightgbm.png}

\column{0.5\textwidth}
\textbf{Performance Metrics:}
\begin{itemize}
    \item \textbf{Accuracy:} 62.94\%
    \item \textbf{F1 Score:} 0.6286
    \item \textbf{Precision:} 0.6299
    \item \textbf{Recall:} 0.6294
\end{itemize}

\vspace{0.3cm}

\textbf{Model Characteristics:}
\begin{itemize}
    \item Training: 80,000 samples
    \item Validation: 20,000 samples
    \item True Positives: 9,700
    \item True Negatives: 9,900
    \item False Positives: 2,100
    \item False Negatives: 2,300
\end{itemize}
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[45 sec]} Diving deeper into LightGBM - our champion model.
\item Confusion matrix shows balanced performance: 9,700 true positives, 9,900 true negatives.
\item False positives: 2,100, False negatives: 2,300 - relatively symmetric errors.
\item Precision and recall both at 0.63 - indicates balanced model, not biased toward either class.
\item Trained on 80,000 samples, validated on 20,000 with stratification.
\item This performance makes it production-ready for first-line screening, though human oversight remains essential.
}

\subsection{Deep Learning}
\begin{frame}{Deep Learning: 6 Architectures Explored}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Implemented from Scratch:}
\begin{enumerate}
    \item \textbf{Deep MLP} - 61.79\%
    \begin{itemize}
        \item 4 layers, 63K params
    \end{itemize}
    \item \textbf{Residual Net} - 61.62\%
    \begin{itemize}
        \item Skip connections, 418K params
    \end{itemize}
    \item \textbf{Simple MLP} - 61.61\%
    \item \textbf{Wide \& Deep} - 61.52\%
    \item \textbf{Attention Net} - 61.45\%
    \begin{itemize}
        \item Multi-head, 1.6M params
    \end{itemize}
    \item \textbf{FT-Transformer} - 61.45\%
    \begin{itemize}
        \item BERT-style, only 38K params!
    \end{itemize}
\end{enumerate}

\column{0.5\textwidth}
\textbf{Critical Learnings:}
\begin{itemize}
    \item \textbf{PyTorch} from scratch
    \item \textbf{GPU:} Apple M4 MPS
    \item \textbf{All DL models: ~61.5\%}
    \begin{itemize}
        \item Architecture matters less
        \item Dataset-limited
    \end{itemize}
    \item \textbf{Best Hyperparameters:}
    \begin{itemize}
        \item Batch: 512, Dropout: 0.3
        \item LR: 0.001 + scheduling
        \item Early stopping essential
    \end{itemize}
\end{itemize}

\begin{alertblock}{Big Learning}
\textbf{ML $>$ DL for tabular by 1.15\%}\\
Confirmed research: Tree ensembles beat neural nets on structured data
\end{alertblock}
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[1 min 30 sec]} Now the exciting part - deep learning experiments.
\item Implemented 6 architectures from scratch in PyTorch: Simple MLP, Deep MLP, Residual Networks, Wide \& Deep, Attention Networks, and FT-Transformer.
\item Deep MLP won at 61.79\% with just 63K parameters - proof that bigger isn't always better.
\item Remarkable finding: ALL DL models converged around 61.5\%. From 38K to 1.6M parameters - same result!
\item This proves the dataset ceiling, not architecture, limits performance.
\item FT-Transformer was particularly interesting - BERT-style attention with only 38K parameters matched complex architectures.
\item Best hyperparameters: batch size 512, dropout 0.3, learning rate 0.001 with scheduling, early stopping crucial.
\item Critical conclusion: ML beats DL by 1.15\% on this tabular data - confirming research that tree ensembles dominate structured data.
\item This validates choosing LightGBM for production deployment.
}

\begin{frame}{Best DL Model: Deep MLP Performance}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Architecture:}
\begin{itemize}
    \item \textbf{Type:} Deep Multi-Layer Perceptron
    \item \textbf{Layers:} 4 hidden layers
    \begin{itemize}
        \item 256 $\rightarrow$ 128 $\rightarrow$ 64 $\rightarrow$ 32
    \end{itemize}
    \item \textbf{Parameters:} 63,714
    \item \textbf{Regularization:}
    \begin{itemize}
        \item BatchNorm after each layer
        \item Dropout: 0.3
    \end{itemize}
    \item \textbf{Optimizer:} Adam
    \item \textbf{Learning Rate:} 0.001
\end{itemize}

\column{0.5\textwidth}
\textbf{Performance Metrics:}
\begin{itemize}
    \item \textbf{Accuracy:} 61.79\%
    \item \textbf{F1 Score:} 0.6130
    \item \textbf{Best Val Loss:} 0.6623
    \item \textbf{Training Time:} ~8 minutes
\end{itemize}

\vspace{0.3cm}

\textbf{Key Insights:}
\begin{itemize}
    \item Best among 6 DL architectures
    \item 1.15\% below LightGBM
    \item Architecture depth matters
    \item Regularization essential
    \item Tree ensembles still superior for tabular data
\end{itemize}
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[45 sec]} Deep MLP details - our best deep learning model.
\item Architecture: 4 hidden layers with decreasing neurons - 256, 128, 64, 32.
\item 63,714 parameters total - efficient design.
\item BatchNorm after each layer for stable training, dropout 0.3 for regularization.
\item Adam optimizer with 0.001 learning rate - standard but effective.
\item Achieved 61.79\% accuracy, F1 of 0.613, best validation loss 0.6623.
\item Training took only 8 minutes on Apple M4 MPS.
\item Key insight: Architecture depth helped, but couldn't overcome dataset limitations.
\item Still 1.15\% below LightGBM - reinforces that tree ensembles are superior for tabular data.
}

% Section 4: Implementation & Deployment
\section{Implementation \& Deployment}

\subsection{System Architecture}
\begin{frame}{Full-Stack Implementation \& Deployment}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Technology Stack:}
\begin{itemize}
    \item \textbf{ML:} scikit-learn, LightGBM
    \item \textbf{DL:} PyTorch 2.9.1, Apple MPS
    \item \textbf{Web:} Next.js 14 + React
    \item \textbf{Deployment:} stf.milav.in
\end{itemize}

\vspace{0.3cm}

\textbf{Web Application Features:}
\begin{itemize}
    \item \textbf{Model Dashboard:} All 13 models with specs
    \item \textbf{Live Predictions:} REST API
    \item \textbf{Interactive UI:} Comparison charts
    \item \textbf{Documentation:} Complete GitHub repo
\end{itemize}

\column{0.5\textwidth}
\textbf{Production Deployment:}
\begin{itemize}
    \item Model serving with preprocessing
    \item RESTful API endpoints
    \item Responsive design
    \item Performance visualization
\end{itemize}

\vspace{0.3cm}

\begin{block}{Live Web App}
\textbf{Visit:} \url{https://stf.milav.in}
\begin{itemize}
    \item Browse all models
    \item View hyperparameters \& metrics
    \item Test live predictions
    \item Access source code
\end{itemize}
\end{block}
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[1 min]} Beyond research - we built a production system.
\item Technology stack: scikit-learn and LightGBM for ML, PyTorch 2.9.1 for DL with Apple MPS acceleration.
\item Frontend: Next.js 14 with React for modern, responsive UI.
\item Deployed at stf.milav.in - fully functional web application.
\item Features include: Model dashboard showing all 13 models with complete specifications.
\item Live prediction API - REST endpoints for real-time inference.
\item Interactive comparison charts for visualizing model performance.
\item Complete documentation and source code on GitHub.
\item This demonstrates end-to-end ML engineering: from research to production deployment.
\item Visit the site to explore models, test predictions, and view the complete implementation.
}

% Section 5: Conclusion
\section{Conclusion}

\subsection{Key Findings}
\begin{frame}{Key Findings \& Insights}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Model Performance:}
\begin{itemize}
    \item \textbf{LightGBM:} 62.94\% (Best)
    \begin{itemize}
        \item F1: 0.6286, Precision: 0.6299
    \end{itemize}
    \item \textbf{Deep MLP:} 61.79\% (Best DL)
    \item \textbf{Kaggle Top:} 69.6\%
    \item \textbf{Gap:} 6.7\% indicates high irreducible error
\end{itemize}

\vspace{0.3cm}

\textbf{Technical Insights:}
\begin{itemize}
    \item ML outperforms DL for tabular data
    \item Weak correlations limit all models
    \item \textbf{FT-Transformer:} Promising - longer training gave better scores, but hardware/time limited full exploration
\end{itemize}

\column{0.5\textwidth}
\textbf{Practical Implications:}
\begin{itemize}
    \item \textbf{Real-world deployment:}
    \begin{itemize}
        \item 62.94\% accuracy
        \item Needs human oversight
        \item First-line screening
    \end{itemize}
    \item \textbf{Production app:} stf.milav.in
    \begin{itemize}
        \item Model dashboard
        \item Live predictions
        \item Complete documentation
    \end{itemize}
\end{itemize}

\vspace{0.3cm}

\textbf{Contributions:}
\begin{itemize}
    \item 13 models evaluated
    \item FT-Transformer implemented
    \item Full-stack deployment
    \item Reproducible pipeline
\end{itemize}
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[1 min 15 sec]} Summarizing our key findings.
\item LightGBM achieved 62.94\% accuracy with F1 of 0.6286 - our best performer.
\item Deep MLP reached 61.79\% - best among DL architectures.
\item Gap to Kaggle top score: 6.7\% - this gap represents better feature engineering, not fundamentally different approaches.
\item Technical insight: ML outperforms DL for tabular data - validates extensive research in this area.
\item Weak correlations fundamentally limit all models - this is a dataset quality issue, not a modeling issue.
\item Practical deployment: 62.94\% accuracy is production-ready for first-line screening with human oversight.
\item Our web application at stf.milav.in demonstrates complete implementation.
\item Contributions: 13 models evaluated, FT-Transformer implemented, full-stack deployment, reproducible pipeline.
\item Note on FT-Transformer: showed promise with longer training but hardware and time constraints limited full exploration.
}

\subsection{Challenges \& Limitations}
\begin{frame}{Challenges, Limitations \& Future Work}
\begin{columns}[T]
\column{0.5\textwidth}
\begin{alertblock}{Key Limitations}
\begin{itemize}
    \item \textbf{Dataset Quality:}
    \begin{itemize}
        \item High irreducible error
        \item Weak features (max corr: 0.118)
        \item Missing critical data
    \end{itemize}
    \item \textbf{Performance Ceiling:}
    \begin{itemize}
        \item Our: 62.94\%, Top: 69.6\%
        \item 6.7\% gap from better features
    \end{itemize}
    \item \textbf{Deployment:}
    \begin{itemize}
        \item 37\% error rate
        \item Requires human oversight
    \end{itemize}
\end{itemize}
\end{alertblock}

\column{0.5\textwidth}
\textbf{Future Enhancements:}
\begin{itemize}
    \item \textcolor{green}{$\checkmark$} DL integration complete
    \item \textbf{Short-term:}
    \begin{itemize}
        \item Explainable AI (SHAP)
        \item Hybrid ML-DL ensembles
        \item Cost-sensitive learning
    \end{itemize}
    \item \textbf{Long-term:}
    \begin{itemize}
        \item Real-time deployment
        \item Multi-class detection
        \item Transfer learning
        \item Edge deployment
    \end{itemize}
\end{itemize}
\end{columns}
\end{frame}

\note[itemize]{
\item \textbf{[1 min]} Being honest about limitations and future directions.
\item Key limitation: Dataset quality with high irreducible error and weak features.
\item Performance ceiling around 63\% - the 6.7\% gap to top Kaggle score comes from better feature engineering.
\item Deployment consideration: 37\% error rate means human oversight is essential.
\item Good news: DL integration complete - we've explored modern architectures.
\item Short-term enhancements planned: Explainable AI using SHAP for interpretability.
\item Hybrid ML-DL ensembles could push performance higher.
\item Cost-sensitive learning for imbalanced scenarios.
\item Long-term vision: Real-time deployment for live threat detection.
\item Multi-class detection for identifying specific malware types.
\item Transfer learning from larger security datasets.
\item Edge deployment for resource-constrained environments.
}

\begin{frame}{Resources}
\begin{block}{Project Resources}
\textbf{Kaggle Competition \& Data:}\\
\url{https://www.kaggle.com/competitions/System-Threat-Forecaster/}

\vspace{0.3cm}

\textbf{Git Repository:}\\
\url{https://github.com/milavdabgar/qip-project-stf}

\vspace{0.3cm}

\textbf{Next.js Web App:}\\
\url{https://stf.milav.in}
\end{block}
\end{frame}

\note[itemize]{
\item \textbf{[15 sec]} All resources are publicly available.
\item Kaggle competition page for data and leaderboard.
\item GitHub repository with complete source code.
\item Live web application at stf.milav.in for hands-on exploration.
\item Feel free to explore, fork, and build upon this work.
}

% Thank You Slide
\begin{frame}
\begin{center}
{\Huge Thank You!}

\vspace{1cm}

{\Large Questions?}

\vspace{1cm}

\textbf{Milav Jayeshkumar Dabgar}\\
Government Polytechnic Palanpur\\
Department of Electronics and Communication Engineering

\end{center}
\end{frame}

\note[itemize]{
\item \textbf{[30 sec]} Thank you for your attention.
\item To summarize: We implemented 13 models, deployed a production web app, and confirmed that ML beats DL for tabular data.
\item I'm happy to answer any questions about the methodology, results, or deployment.
\item Questions to anticipate: Why not neural nets? Dataset limitations. Future work? Explainable AI. Production readiness? Yes, with human oversight.
}

\end{document}
