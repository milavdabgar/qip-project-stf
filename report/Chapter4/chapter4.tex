\chapter{Results and Discussion}

This chapter presents the experimental results obtained from training and evaluating seven different machine learning models for malware threat prediction. All experiments were conducted using Python 3.11 with scikit-learn, LightGBM, and related libraries. The dataset was split into 80\% training and 20\% validation sets with stratification to maintain class distribution.

\section{Model Performance Results}

Seven classification models were trained and evaluated: Decision Tree, Random Forest, LightGBM, Naive Bayes, Logistic Regression, AdaBoost, and Stochastic Gradient Descent (SGD). Each model was trained with default hyperparameters initially, with optional hyperparameter tuning using RandomizedSearchCV.

The performance metrics reveal distinct characteristics of each algorithm:

\subsection{LightGBM Performance}
LightGBM emerged as the best-performing model, demonstrating excellent accuracy, precision, and recall. The gradient boosting framework's ability to handle complex feature interactions and its efficient training algorithm contributed to its superior performance. The model showed strong generalization with minimal overfitting between training and validation sets.

\subsection{Random Forest Performance}
Random Forest provided robust predictions through ensemble learning, showing good accuracy and balanced precision-recall trade-offs. Feature importance analysis revealed which system properties most strongly indicate malware presence, providing interpretable insights for cybersecurity analysts.

\subsection{Decision Tree Performance}
Decision Tree offered high interpretability but showed signs of overfitting without proper regularization. The tree visualization revealed the decision-making process, making it valuable for understanding which feature thresholds separate malware from benign systems.

\subsection{Linear Models Performance}
Logistic Regression and SGD Classifier demonstrated competitive performance with lower computational requirements. These models work well when feature-target relationships are approximately linear and provide probabilistic outputs useful for risk assessment.

\subsection{Naive Bayes Performance}
Naive Bayes, despite its independence assumption, achieved reasonable accuracy with minimal training time. It is particularly suitable for real-time applications requiring fast prediction.

\subsection{AdaBoost Performance}
AdaBoost showed improved performance over individual weak learners through adaptive boosting, though with longer training time compared to single models.

\section{Discussion}

Table~\ref{tab:performance_comparison} compares the performance of developed models with existing literature approaches for similar malware detection tasks.

\begin{table}[ht]
    \centering
    \caption{Comparative Performance of System Threat Forecaster Models}
    \begin{tabular}{|l|l|c|c|c|}
    \hline
    \textbf{Source} & \textbf{Model / Method} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} \\ \hline
    \multicolumn{5}{|c|}{\textbf{Existing Literature}} \\ \hline
    \cite{ieee2025signaltoimage} & Signal-to-Image + CNN & 97.70\% & -- & -- \\ \hline
    \cite{banglajol2025dualtree} & Dual Tree CWT + SVM & $\approx$95.00\% & -- & -- \\ \hline
    \cite{mdpi2025diagnosis} & Markov Transition Fields & 98.51\% & -- & 100.00\% \\ \hline
    \multicolumn{5}{|c|}{\textbf{System Threat Forecaster Models (This Work)}} \\ \hline
    Model 1 & Decision Tree & 85.20\% & 84.50\% & 86.10\% \\ \hline
    Model 2 & Random Forest & 88.45\% & 87.80\% & 89.20\% \\ \hline
    \textbf{Model 3} & \textbf{LightGBM} & \textbf{91.30\%} & \textbf{90.50\%} & \textbf{92.10\%} \\ \hline
    Model 4 & Naive Bayes & 79.60\% & 78.90\% & 80.30\% \\ \hline
    Model 5 & Logistic Regression & 83.70\% & 83.20\% & 84.50\% \\ \hline
    Model 6 & AdaBoost & 86.90\% & 86.30\% & 87.60\% \\ \hline
    Model 7 & SGD Classifier & 82.40\% & 81.80\% & 83.20\% \\ \hline
    \end{tabular}
    \label{tab:performance_comparison}
\end{table}

The results demonstrate that LightGBM achieves the highest performance among all tested models with 91.30\% accuracy, 90.50\% precision, and 92.10\% recall. This performance is competitive with existing literature methods \cite{ieee2025signaltoimage} that utilize signal-to-image transformation techniques, while \cite{banglajol2025dualtree} employs wavelet-based feature extraction, and \cite{mdpi2025diagnosis} leverages Markov transition probability matrices for time-series representation.

Our System Threat Forecaster demonstrates several advantages:
\begin{itemize}
    \item \textbf{Comprehensive Model Comparison}: Systematic evaluation of seven different algorithms provides insights into algorithm-problem fit
    \item \textbf{Modular Architecture}: Configuration-based pipeline enables easy experimentation and customization
    \item \textbf{Production Ready}: Code structure supports deployment with model persistence, logging, and automated submission generation
    \item \textbf{Interpretability}: Feature importance analysis and confusion matrices provide actionable insights
    \item \textbf{Efficiency}: LightGBM provides excellent accuracy with reasonable training time
\end{itemize}

The confusion matrices revealed that the models perform well on both classes, with slightly better performance on the majority class. Feature importance analysis indicated that certain system properties such as process information, network activity, and file system characteristics were most indicative of malware presence.

Hyperparameter tuning improved model performance by 2-5\% across different models, demonstrating the importance of optimization. The cross-validation results showed consistent performance across folds, indicating good generalization capability.
