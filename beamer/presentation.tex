% System Threat Forecaster Presentation
\documentclass[aspectratio=169]{beamer}

% Theme
\usetheme{Madrid}
\usecolortheme{default}

% Packages
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}

% Title Information
\title{System Threat Forecaster}
\subtitle{AICTE QIP PG Certification Programme on\\``Deep Learning: Fundamentals and Applications''}
\author{Milav Jayeshkumar Dabgar}
\institute{%
    Department of Electronics Engineering\\
    Sardar Vallabhbhai National Institute of Technology, Surat
}
\date{December 2025}

\begin{document}

% Title Slide
\begin{frame}
\titlepage
\end{frame}

% Outline
\begin{frame}{Outline}
\tableofcontents
\end{frame}

% Section 1: Introduction
\section{Introduction}

\begin{frame}{Background}
\begin{itemize}
    \item Cybersecurity threats are increasingly sophisticated
    \item Malware poses significant risks:
    \begin{itemize}
        \item Data breaches and financial losses
        \item System compromise and data theft
        \item Operational disruptions
    \end{itemize}
    \item Traditional signature-based antivirus solutions struggle with:
    \begin{itemize}
        \item Zero-day attacks and polymorphic malware
        \item Evolving threat landscapes
    \end{itemize}
    \item \textbf{Machine Learning} offers proactive, behavior-based threat detection
\end{itemize}
\end{frame}

\begin{frame}{Problem Statement}
\begin{block}{Primary Challenge}
Predict malware infections based on system properties using 100,000 samples with 76 diverse features
\end{block}

\vspace{0.3cm}

\textbf{Specific Challenges:}
\begin{itemize}
    \item High dimensionality: 48 numerical + 28 categorical features
    \item Missing values in critical features (RealTimeProtectionState, CityID)
    \item Balanced but complex dataset (50.52\% positive, 49.48\% negative)
    \item Need for efficient and accurate classification
\end{itemize}
\end{frame}

% Section 2: Objectives
\section{Objectives}

\begin{frame}{Project Objectives}
\begin{enumerate}
    \item \textbf{Data Preprocessing}: Implement comprehensive preprocessing techniques
    \begin{itemize}
        \item Missing value imputation
        \item Feature encoding and normalization
    \end{itemize}
    
    \item \textbf{Feature Engineering}: Develop strategies to enhance model performance
    
    \item \textbf{Model Development}: Train and evaluate multiple ML models
    
    \item \textbf{Performance Optimization}: Hyperparameter tuning and model selection
    
    \item \textbf{Model Comparison}: Systematic evaluation using standard metrics
    
    \item \textbf{Deployment Ready}: Create maintainable, production-ready codebase
\end{enumerate}
\end{frame}

% Section 3: Methodology
\section{Methodology}

\begin{frame}{Data Processing Pipeline}
\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node (data) [rectangle, draw, fill=blue!20, text width=2.5cm, text centered, minimum height=1cm] {Raw Data};
    \node (preprocess) [rectangle, draw, fill=green!20, text width=2.5cm, text centered, minimum height=1cm, below of=data] {Preprocessing};
    \node (feature) [rectangle, draw, fill=yellow!20, text width=2.5cm, text centered, minimum height=1cm, below of=preprocess] {Feature Engineering};
    \node (model) [rectangle, draw, fill=orange!20, text width=2.5cm, text centered, minimum height=1cm, below of=feature] {Model Training};
    \node (evaluate) [rectangle, draw, fill=red!20, text width=2.5cm, text centered, minimum height=1cm, below of=model] {Evaluation};
    
    \draw[->, thick] (data) -- (preprocess);
    \draw[->, thick] (preprocess) -- (feature);
    \draw[->, thick] (feature) -- (model);
    \draw[->, thick] (model) -- (evaluate);
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}{Dataset Characteristics}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Dataset Overview:}
\begin{itemize}
    \item \textbf{Size:} 100,000 samples
    \item \textbf{Features:} 76 total
    \begin{itemize}
        \item 48 numerical
        \item 28 categorical
    \end{itemize}
    \item \textbf{Split:} 80\% train, 20\% validation
\end{itemize}

\column{0.5\textwidth}
\textbf{Preprocessing Pipeline:}
\begin{itemize}
    \item Missing values:
    \begin{itemize}
        \item Mean for numerical
        \item Most frequent for categorical
    \end{itemize}
    \item Label Encoding for categoricals
    \item StandardScaler normalization
    \item Stratified splitting
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{Machine Learning Models}
\begin{block}{Seven Classification Algorithms Evaluated}
\begin{enumerate}
    \item \textbf{Decision Tree} - High interpretability
    \item \textbf{Random Forest} - Ensemble method
    \item \textbf{LightGBM} - Gradient boosting framework
    \item \textbf{Naive Bayes} - Probabilistic classifier
    \item \textbf{Logistic Regression} - Linear baseline
    \item \textbf{AdaBoost} - Adaptive boosting
    \item \textbf{SGD Classifier} - Stochastic optimization
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Key Features Analysis}
\textbf{Most Predictive Features (Correlation with Target):}
\begin{enumerate}
    \item \textbf{AntivirusConfigID} -- Correlation: 0.118
    \item \textbf{TotalPhysicalRAMMB} -- Correlation: 0.066
    \item \textbf{IsSystemProtected} -- Correlation: 0.062
    \item \textbf{IsGamer} -- Correlation: 0.061
\end{enumerate}

\vspace{0.3cm}
\textbf{Feature Engineering Configuration:}
\begin{itemize}
    \item Interaction terms available (currently disabled)
    \item SelectKBest with f\_classif (top 30 features)
    \item Optional PCA for dimensionality reduction
\end{itemize}

\vspace{0.2cm}
\begin{alertblock}{Key Insight}
Security configuration and system specifications show moderate but consistent predictive power
\end{alertblock}
\end{frame}

% Section 4: Results
\section{Results}

\begin{frame}{Model Performance Comparison}
\begin{table}
\centering
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Rank} \\
\midrule
\textbf{LightGBM} & \textbf{63.0\%} & \textbf{1st} \\
Random Forest & 62.4\% & 2nd \\
AdaBoost & 62.0\% & 3rd \\
Logistic Regression & $\sim$60\% & 4th \\
Decision Tree & $\sim$60\% & 5th \\
SGD Classifier & $\sim$60\% & 6th \\
Naive Bayes & <60\% & 7th \\
\bottomrule
\end{tabular}
\end{table}

\vspace{0.2cm}
\begin{block}{Key Observation}
LightGBM achieved the highest accuracy at \textbf{63.0\%}, showing gradient boosting's superiority for this complex classification task
\end{block}

\vspace{0.1cm}
\footnotesize{Note: Ensemble methods (Random Forest, AdaBoost) outperformed traditional classifiers}
\end{frame}

\begin{frame}{Key Findings}
\begin{itemize}
    \item \textbf{Model Performance}: 63\% accuracy ceiling suggests complex relationships
    \begin{itemize}
        \item Moderate feature correlations (max 0.118)
        \item Room for feature engineering improvements
    \end{itemize}
    \item \textbf{Ensemble Superiority}: Tree-based ensembles (LightGBM, RF, AdaBoost) outperformed linear models
    \item \textbf{Dataset Challenges}:
    \begin{itemize}
        \item Missing data in important features
        \item Balanced classes but complex decision boundaries
    \end{itemize}
    \item \textbf{Feature Insights}: Security configurations more predictive than hardware specs
\end{itemize}
\end{frame}

\begin{frame}{Implementation Highlights}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Architecture Strengths:}
\begin{itemize}
    \item Configuration-driven pipeline
    \item Modular design pattern
    \item 7 ML models implemented
    \item Automated hyperparameter tuning
    \item Model persistence with joblib
\end{itemize}

\column{0.5\textwidth}
\textbf{Pipeline Features:}
\begin{itemize}
    \item Robust preprocessing
    \item Missing value handling
    \item Feature engineering tools
    \item Cross-validation support
    \item Comprehensive logging
    \item Results tracking
\end{itemize}
\end{columns}
\end{frame}

% Section 5: Implementation
\section{Implementation}

\begin{frame}{System Architecture}
\textbf{13-Module Pipeline Structure:}
\begin{enumerate}
    \item Environment Setup \& Configuration
    \item Data Loading \& Utilities (save/load models)
    \item Exploratory Data Analysis (EDA)
    \item Data Preprocessing (imputation, encoding, scaling)
    \item Feature Engineering (interactions, selection)
    \item Model Training (7 algorithms with tuning)
    \item Model Evaluation \& Comparison
    \item Prediction \& Submission Generation
\end{enumerate}

\vspace{0.2cm}
\textbf{Technology Stack:}
\begin{itemize}
    \item Python 3.x, scikit-learn 1.3+, LightGBM
    \item pandas, numpy, matplotlib, seaborn, joblib
\end{itemize}
\end{frame}

\begin{frame}{Code Organization}
\begin{block}{Configuration-Driven Design}
Selective enabling/disabling of:
\begin{itemize}
    \item Preprocessing steps
    \item Feature engineering techniques
    \item Model selection
    \item Hyperparameter tuning
\end{itemize}
\end{block}

\vspace{0.3cm}

\textbf{Benefits:}
\begin{itemize}
    \item Easy experimentation
    \item Maintainable codebase
    \item Model persistence and logging
    \item Reproducible results
\end{itemize}
\end{frame}

% Section 6: Conclusion
\section{Conclusion}

\begin{frame}{Key Contributions}
\begin{enumerate}
    \item \textbf{Comprehensive Model Comparison}: Systematic evaluation of 7 classification algorithms
    
    \item \textbf{Production-Ready Pipeline}: 13-module architecture with configuration control
    
    \item \textbf{Robust Preprocessing}: Handles 76 features (48 numerical, 28 categorical)
    
    \item \textbf{Automated Workflows}: Hyperparameter tuning, cross-validation, model persistence
    
    \item \textbf{Best-in-Class Performance}: 63\% accuracy with LightGBM on 100K samples
    
    \item \textbf{Reproducible Results}: Complete logging and results tracking system
\end{enumerate}
\end{frame}

\begin{frame}{Practical Implications}
\begin{itemize}
    \item \textbf{Early Threat Detection}: Proactive malware identification
    \item \textbf{Scalability}: Efficient algorithms for large-scale deployment
    \item \textbf{Interpretability}: Feature importance for analyst understanding
    \item \textbf{Flexibility}: Multiple models for different requirements
    \item \textbf{Cost-Effectiveness}: Automated detection reduces manual effort
\end{itemize}
\end{frame}

\begin{frame}{Future Work}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Technical Enhancements:}
\begin{itemize}
    \item Deep learning integration
    \item Real-time deployment
    \item Ensemble methods
    \item Explainable AI (SHAP, LIME)
    \item Active learning
\end{itemize}

\column{0.5\textwidth}
\textbf{Practical Extensions:}
\begin{itemize}
    \item Multi-class classification
    \item SIEM integration
    \item Adversarial robustness
    \item Transfer learning
    \item Automated feature engineering
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{Challenges \& Limitations}
\begin{alertblock}{Key Challenges Identified}
\begin{itemize}
    \item \textbf{Performance Ceiling}: 63\% accuracy indicates complex relationships
    \item \textbf{Feature Correlations}: Weak correlations (max 0.118) limit predictive power
    \item \textbf{Missing Data}: RealTimeProtectionState, CityID have significant gaps
    \item \textbf{Feature Engineering}: Currently disabled, potential for improvement
    \item \textbf{Model Complexity}: Balance between accuracy and interpretability
\end{itemize}
\end{alertblock}

\vspace{0.2cm}
\textbf{Future Improvements:}
\begin{itemize}
    \item Enable and optimize feature engineering
    \item Advanced imputation strategies
    \item Deep learning exploration
\end{itemize}
\end{frame}

\begin{frame}{Resources}
\begin{block}{Project Resources}
\textbf{Kaggle Competition:}\\
\url{https://www.kaggle.com/competitions/System-Threat-Forecaster/}

\vspace{0.3cm}

\textbf{Implementation Notebook:}\\
\url{https://www.kaggle.com/code/milavdabgar/system-threat-forecaster-modular}
\end{block}

\vspace{0.3cm}

\textbf{Technologies:} Python 3.11, scikit-learn, LightGBM, pandas, numpy, matplotlib
\end{frame}

% Thank You Slide
\begin{frame}
\begin{center}
{\Huge Thank You!}

\vspace{1cm}

{\Large Questions?}

\vspace{1cm}

\textbf{Milav Jayeshkumar Dabgar}\\
Government Polytechnic Palanpur\\
Department of Computer Engineering

\end{center}
\end{frame}

\end{document}
