% conclusion.tex

\chapter{Conclusion and Future Work}

\section{Key Findings}

LightGBM achieved 62.94\% accuracy, outperforming Deep MLP (61.79\%) by 1.15\%. All six DL architectures clustered within 0.34\% (61.45-61.79\%), demonstrating that complexity (38K-1.6M parameters) provides negligible benefit. ML trained 4x faster with simpler deployment and better interpretability.

\textbf{Why ML Wins}: Tabular structure (no spatial/temporal patterns), moderate sample size (80K), weak correlations (max 0.118), heterogeneous types favor tree-based methods with strong inductive bias.

\section{Contributions}

Empirical evidence that gradient boosting outperforms sophisticated DL for tabular cybersecurity data. Clear decision criteria based on data characteristics. Reproducible framework with production deployment at \url{https://stf.milav.in}.

\section{Practical Recommendations}

Use tree ensembles (LightGBM/XGBoost) for tabular data with <1M samples, heterogeneous types, and weak correlations. Reserve DL for rich structured patterns with sufficient training data. Prioritize data quality over model sophistication.

\section{Limitations}

Single dataset (weak correlations max 0.118), minimal feature engineering, individual models only (no ensembling), limited hyperparameter tuning due to compute constraints.

\section{Future Work}

\textbf{Feature Engineering}: Interaction terms, polynomial features, domain transformations to approach Kaggle 69.6\% ceiling.

\textbf{Ensembles}: Stack ML models (LightGBM + XGBoost + CatBoost), blend predictions.

\textbf{Advanced DL}: TabNet, SAINT, NODE architectures; extended training (200+ epochs).

\textbf{Explainability}: SHAP values, LIME analysis, feature importance visualization.

\textbf{Cross-Domain}: Validate on network intrusion, phishing datasets.

\textbf{Production}: Real-time API, monitoring, A/B testing, continuous learning.

\section{Concluding Remarks}

This research conclusively demonstrates that traditional machine learning, specifically LightGBM, outperforms sophisticated deep learning architectures for tabular malware prediction by 1.15\%. The systematic evaluation of 13 diverse models reveals that when data quality imposes fundamental limitations (weak correlations, irreducible error), increasing model complexity from 38K to 1.6M parameters yields negligible improvement (0.34\%).

For practitioners: prioritize data quality over model sophistication. For tabular cybersecurity data with moderate sample sizes, tree-based ensembles provide superior accuracy, faster training, simpler deployment, and crucial interpretability. Deep learning's advantages in vision and NLP do not transfer to heterogeneous tabular data lacking spatial or temporal structure.

The deployed web application at \url{https://stf.milav.in} demonstrates practical implementation, while the reproducible codebase and comprehensive documentation enable researchers to build upon this foundation. Future work should explore advanced feature engineering, model ensembles, and cross-domain validation to further establish the boundaries of ML and DL effectiveness for cybersecurity applications.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../imgs/next-home.png}
\caption{Deployed Web Application: Homepage showcasing System Threat Forecaster with 7 ML and 6 DL models for real-time malware prediction.}
\label{fig:webapp_home}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../imgs/next-models.png}
\caption{Model Performance Dashboard: Interactive comparison of all 13 models with detailed metrics and hyperparameters.}
\label{fig:webapp_models}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../imgs/next-predict.png}
\caption{Prediction Interface: User-friendly form accepting 75 features for real-time threat prediction using all trained models.}
\label{fig:webapp_predict}
\end{figure}
